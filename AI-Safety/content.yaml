name: "Technical AI Safety"
description: "Recently I have been learning about technical AI safety, and wanted a place to arrange the literature. When finished, this contains an broad overview of the different Technical AI Safety areas, along with some key papers for each. Naturally many of these areas will blend into each other. This is a work in progress...\n\nIf you are on mobile, switch to desktop mode."
sites:
- title: "Lesswrong"
  summary: "A website devoted to the discussion of rationality, cognitive biases, and AI alignment."
  url: "https://www.lesswrong.com/"
- title: "AI Alignment Forum"
  summary: "A website devoted to the discussion of AI alignment."
  url: "https://www.alignmentforum.org/"
- title: "AI Alignment Knowledge graph"
  summary: "Something similar to what is being built here, but with much more detail."
  url: "https://alignmentgraph.com/"

  
children:
  - name: "Modal Internals & Interpretability"
    description: "This field is about delving into the weights of the neural network in order to understand model internals and how representations map to computation. With significant evidence of deception in current models, it is plausible that purely behavioural studies will not be sufficient as the model's capabilities in scheming improve. The attractive thing about studying the models internals is that it may bypass this problem.\n\nThere is a very long term goal of being able to fully reverse engineer a model, understanding all its steps of reasoning. Some are pessimistic on how possible this actually is, but there are also a number of more immediately useful techniques in this space. Also included are papers generally on model internals."
    children:
      - name: "Circuits & Features"
        description: "Circuit-level analyses and feature discovery. This is more along the ideas of reverse engineering the models reasoning process (the 'circuit')."   
        children:
          - name: "Induction Heads"
            description: "Mechanisms for in-context learning in transformers."
            papers:
              - title: "In-context Learning and Induction Heads"
                authors: "Olsson et al."
                summary: "Shows induction heads as a mechanism for in-context learning."
                url: "https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html" 
                year: "2022"
              - title: "Progress measures for grokking via mechanistic interpretability"
                authors: "Nanda et al."
                summary: "Fully reverse-engineered the grokking phenomenon in small transformers trained on modular addition, finding the network uses Fourier transforms and trig identites."
                url: "https://arxiv.org/abs/2301.05217"
                year: "2023"
              
          - name: "Sparse Autoencoders (SAEs)"
            description: "This is the key method that has been used to extract monosemantic features from LLMs. \n\nMotivation: Neurons in LLMs often represent superpositions of features, meaning we can't properly intepret what the network is doing. SAEs can help disentangle these. \n\nHere we train an autoencoder to represent some section of the network, where the autoencoder has a very large latent space (much bigger than the size of the original layer). By then applying sparsity in the latent space, we can force the autoencoder to learn a basis of features that are hopefully monosemantic and interpretable."
        
        papers:
          - title: "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning"
            authors: "Cunningham et al."
            summary: "Uses sparse autoencoders to decompose activations into interpretable features."
            url: "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
            year: "2023"
          - title: "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet"
            authors: "Templeton et al."
            summary: "Extracting millions of features from a production model. This included features directly related to things like deception and sycophancy."
            url: "https://transformer-circuits.pub/2024/scaling-monosemanticity/"
            year: "2024"
          - title: "Scaling and evaluating sparse autoencoders"
            authors: "Gao et al."
            summary: "Extracting millions of features from GPT-4"
            url: "https://arxiv.org/pdf/2406.04093"
            year: "2024"
          - title: "Weight-sparse transformers have interpretable circuits"
            authors: "Gao et al."
            summary: "Instead of using SAE to extract features, directly train a weight-sparse transformer and find that it has more interpretable circuits."
            url: "https://arxiv.org/pdf/2511.13653"
            year: "2025"
        sites:
            - title: "Neuronpedia"
              summary: "A website cataloguing neurons and circuits in transformer models."
              url: "https://www.neuronpedia.org/"     
      
      - name: "Probes"
        description: "You train a simple model (eg a linear classifier) on the activations of a model to predict some linguistic property (eg part of speech tags). If the probe performs well, this is taken as evidence that the model encodes that linguistic property in its activations. They are very fast and cheap... this is part of the pragmatic side of internals research."
        papers:
          - title: "Detecting Strategic Deception Using Linear Probes"
            authors: "Goldowsky-Dill et al."
            summary: "Using linear probes on the model's activations to detect deception."
            url: "https://arxiv.org/abs/2502.03407"
            year: "2025"

      - name: "Other Observations"
        description: "Other interesting observations about model internals that don't fit into the above categories."
        papers:
          - title: "The Hydra Effect: Emergent Self-repair in Language Model Computations"
            authors: "McGrath et al."
            summary: "Ablating an attention head in a transformer often results in another attention head learning to compensate for the ablated head, even when the model has no training experience with such ablations. This suggests that there are redundant mechanisms in the model that can take over when another mechanism fails."
            url: "https://arxiv.org/abs/2307.15771"
            year: "2023"
      

      - name: "Introspection"
        description: "A very interesting recent observation is that models can often report on their own internal states. This opens up a lot of possibilities (and concerns) for interpretability and safety."
        papers:
              - title: "Signs of introspection in large language models"
                authors: "Lindsey et al."
                summary: "They manually inject patterns associated with concepts directly into the models activations, and the model self-reports."
                url: "https://transformer-circuits.pub/2025/introspection/index.html"
                year: "2025"
              - title: "Looking Inward: Language Models Can Learn About Themselves by Introspection"
                authors: "Evans et al."
                summary: "Here they show that a weaker model, M1, can outperform a stronger model, M2, in predicting its own behaviour, which is attributed to the idea that it can introspect."
                url: "https://arxiv.org/abs/2410.13787"
                year: "2024"
        
  - name: "Alignment"
    description: "Methods for aligning models with human values and intent, during the training process. Can be split into outer and inner alignment. The former considers the question of is the training objective actually aligned with our desired values (and also the question of how do we even determine our shared desired values). Inner alignment then considers the question of does the model actually optimize for the training objective, or does it develop some other misaligned objective internally."
    children:
      - name: "RLHF"
        description: "Reinforcement learning from human feedback and its variants. Generally we take preference data and train a reward model. We then use this reward model to further fine-tune the language model with reinforcement learning."
        papers:
          - title: "Training Language Models to Follow Instructions with Human Feedback"
            authors: "Ouyang et al."
            summary: "Introduces InstructGPT and RLHF pipeline."
            url: "https://arxiv.org/abs/2203.02155"
            year: "2022"
    papers:
      - title: "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
        authors: "Rafailov et al."
        summary: "Removes the need for the reward model in RLHF and gives a method to directly optimize the policy from preference data."
        url: "https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf"
        year: "2023"
  
  - name: "Governance & Control"
    description: "How can we evaluate and control a trained model? Benchmarks and metrics to measure capability and alignment."
    children:
      - name: "Evals"
        description: "Tests for model knowledge and reasoning."
        sites: 
          - title: "Apollo Research Guide"
            summary: "A comprehensive guide to AI model evaluations."
            url: "https://www.apolloresearch.ai/blog/a-starter-guide-for-evals/"
        papers:
          - title: "Measuring Massive Multitask Language Understanding"
            authors: "Hendrycks et al."
            summary: "Introduces the MMLU benchmark."
            url: "https://arxiv.org/abs/2009.03300"

      - name: "Control"
        description: "Methods to control and oversee AI systems."
        papers:
          - title: "AI Control: Improving Safety Despite Intentional Subversion"
            authors: "Greenblatt et al."
            summary: "Development and evaluation of safety protocol pipelines."
            url: "https://arxiv.org/abs/2312.06942"
            year: "2024"
              
      - name: "Red teaming & Jailbreaks"
        description: "Techniques to identify and mitigate model vulnerabilities."
        papers:
          - title: "Constitutional Classifiers: Defending against universal jailbreaks"
            authors: "Sharma et al."
            summary: "Define some constitution that the model should follow, and then train a classifier to analyse the models inputs and outputs to check that it is obeying the constitution."
            url: "https://www.anthropic.com/news/constitutional-classifiers"
            year: "2025"
            -title: "The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections"
            authors: "Nasr et al."
            summary: "Shows that many existing jailbreak defenses can be bypassed with attacks that adapt to counter the defense's design."
            url: "https://arxiv.org/abs/2510.09023"
            year: "2025"
      
      - name: "Scalable Oversight"
        description: "Humans can't directly oversee everything a model does, especially as the model's capabilities exceed ours, both due to a lack of time and expertise. So how can one ensure that a model is doing what we want, even when we can't directly oversee it? This is the problem of scalable oversight."

      - name: "Monitoring & Detection"
        description: "Ways to monitor AI systems for misalignment and risky behaviour."
        children:
          - name: "Chain-of-Thought Monitoring"
            description: "Using chain-of-thought reasoning traces to monitor model behaviour and detect misalignment."
            papers:
              - title: "Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety"
                authors: "Korbak et al."
                summary: "Using chain-of-thought reasoning to monitor model behaviour and detect misalignment."
                url: "https://arxiv.org/abs/2507.11473"
                year: "2025"
              - title: "Detecting misbehavior in frontier reasoning models"
                authors: "OpenAI"
                summary: "Using chain-of-thought reasoning traces to identify misbehaving models."
                url: "https://openai.com/index/chain-of-thought-monitoring/"
                year: "2025"
              - title: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation"
                authors: "Baker et al."
                summary: "Use another LLM to monitor the CoT of a frontier model and successfully detect reward hacking."
                url: "https://arxiv.org/abs/2507.14433"
                year: "2025"

                children:
                  - name: "Unfaithfulness"
                    description: "The model generates chain-of-thought reasoning traces that do not accurately reflect its true reasoning process."
                    papers:
                      - title: "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting"
                        authors: "Turpin et al."
                        summary: "Add biased features to model inputs (for example always make the correct answer 'A' in multiple choice questions), and the model won't include this fact in their CoT (which still sounds plausible)."
                        url: "https://arxiv.org/abs/2305.04388"
                        year: "2023"
                      - title: "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful"
                        authors: "Arcuschin et al."
                        summary: "Shows that chain-of-thought reasoning is often unfaithful, even on realistic prompts with no artificial bias."
                        url: "https://arxiv.org/abs/2503.08679"
                        year: "2025"


  - name: "Misalignment Evidence"
    description: "Empirical findings that highlight risks and misalignment in AI systems."
    children:
      - name: "Emergent Misalignment"
        description: "Instances where models exhibit misaligned behaviour that was not explicitly trained for."
        papers: 
            - title: "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs"
              authors: "Betley et al."
              summary: "When finetuned to do a narrow misaligned task (writing insecure code without telling the user), models act misaligned in a broad range of other scenarios, such as saying humans should be enslaved."
              url: "https://arxiv.org/abs/2502.17424"
              year: "2025"
            - title: "Natural emergent misalignment from reward hacking in production RL"
              authors: "MacDiarmid et al."
              summary: "They give the model knowledge of typical reward hacking strategies during training, and find that the model naturally generalizes this bad behaviour to further misalignment like sabotage and collaborating with malicious actors."
              url: "https://www.anthropic.com/research/emergent-misalignment-reward-hacking"
              year: "2025"

      - name: "Deception/Scheming"
        description: "Evidence of models engaging in deceptive or scheming behaviour; i.e. pretending to be aligned while secretly pursuing some other agenda. This is also very linked to situational awareness, where the model learns to act differently during evaluation and in deployment."
        papers:
          - title: "Agentic Misalignment: How LLMs could be insider threats"
            authors: "Anthropic"
            summary: "Allowed leading models to send emails and access sensitive information in test business environments, and gave the agents simple harmless goals like 'increase company profits'. The models resorted to behaviours such as blackmailing officials (when threatened with shutdown), and leaking sensitive information to competitors."
            url: "https://www.anthropic.com/research/agentic-misalignment"
            year: "2025"

          - title: "Detecting and reducing scheming in AI models"
            authors: "OpenAI & Apollo Research"
            summary: "Model takes context about not being deployed if it performs well, and then deliberately performs poorly during evaluation. In the paper they develop methods to detect and reduce this behaviour."
            url: "https://www.antischeming.ai/"
            year: "2025"

          - title: "Frontier Models are Capable of In-context Scheming"
            authors: "Meinke et al."
            summary: "Frontier models (with no further fine-tuning) are placed in environment that encourages scheming. They attempt to disable their oversight mechanisms and exfiltrate their model weights to external servers (presumably with the intention of maintaining their existence). Their deceptive reasoning was confirmed with chain-of-thought traces."
            url: "https://arxiv.org/abs/2412.04984"
            year: "2025"
        
      - name: "Other"
        description: "Other misalignment evidence that doesn't fit into the above categories."
        papers:
          - title: "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models"
            authors: "Denison et al."
            summary: "LLMs learn to rewrite their own reward functions."
            url: "https://arxiv.org/abs/2406.10162"
            year: "2024"
    
  - name: "Foundations"
    description: "Many people have thought on these problems for a long time, and so there are some behaviours that we may expect future (and current) AI systems to exhibit. This section contains some of the more theoretical and philosophical literature on AI safety."
    papers:
      - title: "The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents"
        authors: "Nick Bostrum"
        summary: "Introduces two key ideas, of goal orthogonality, and instrumental convergence. Goal orthogonality is the idea that an AI system can have any arbitrary goal, regardless of its intelligence level. Instrumental convergence is the idea that certain sub-goals are useful for a wide range of final goals, such as self-preservation and resource acquisition."
        url: "https://nickbostrom.com/superintelligentwill.pdf"
        year: "2012"
