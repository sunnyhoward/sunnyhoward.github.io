<p>
    Everyone seems to have a website now, so I thought I'd make one too. I'm Sunny, and I'm an aspiring AI safety researcher. 

    <br><br>

    I recently finished my Physics PhD at Oxford, working on<span class="tooltip">machine learning and ultra-intense physics. <span class="tooltip-text">If anyone wants to work on this intersection, I would highly recommend my supervisors, <a href="https://www.physics.ox.ac.uk/our-people/dopp">Andreas</a> and <a href="https://www.physics.ox.ac.uk/our-people/norreys">Peter</a>.</span> </span>   Before this, I did a short time in Cambridge working on nanophotonics and studied Physics and Machine Learning at Nottingham (BSc and MSc respectively). My CV is <a href="CV.pdf">here</a>. <br><br>

    During my MSc, I read some books on the potential impacts of superintelligent AI, such as Life 3.0 and Human Compatible. It seemed undoubtably important, but also a problem that felt far in the future. In the last few years, LLMs have gone from struggling to make coherent sentences to being able to answer physics problems better than PhD students (yikes). At the same time, they are already exhibiting serious misalignment in test cases, such as <a href="https://www.anthropic.com/research/agentic-misalignment">blackmailing officials</a> and <a href="https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/">behaving differently when they know they're being tested</a>. Thus, in this critical time, it seems that one of the most valuable things one can do is to work to ensure that AI is beneficial for all beings. To organize technical AI safety research, I plan to put together a <a href="AI-Safety/">knowledge graph</a>; maybe it can be useful to others once it's up-to-date. <br><br>

    Otherwise, I like things like Buddhism, climbing and music. 
  </p>